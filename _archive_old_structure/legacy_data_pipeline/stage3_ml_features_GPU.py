"""
Stage 3: Advanced ML Feature Engineering - GPU-ACCELERATED VERSION üöÄ

Uses RAPIDS cuDF for 10-50x speedup on feature calculations
Perfect for 17K+ snapshots with 60+ features

Run from project root: python data_pipeline/stage3_ml_features_GPU.py
"""

import cudf
import cupy as cp
import numpy as np
import pandas as pd
from pathlib import Path
import warnings
import time
warnings.filterwarnings('ignore')


class AdvancedFeatureEngineerGPU:
    """
    GPU-accelerated ML feature engineering using RAPIDS cuDF
    
    Expected speedup: 10-30x over pandas for:
    - Rolling window calculations
    - GroupBy operations
    - Vectorized math operations
    """
    
    def __init__(self, 
                 prediction_horizons=[30, 60, 300],
                 return_thresholds=[0.001, 0.002, 0.005],
                 vpin_window=50,
                 min_spread_filter=True):
        self.prediction_horizons = prediction_horizons
        self.return_thresholds = return_thresholds
        self.vpin_window = vpin_window
        self.min_spread_filter = min_spread_filter
        
    def compute_basic_features(self, df):
        """Compute basic features (GPU-accelerated)"""
        print("\nüîß Computing basic features (GPU)...")
        start = time.time()
        
        # All operations run on GPU
        df['log_mid_price'] = cudf.log(df['mid_price'])
        df['log_spread'] = cudf.log(df['spread'] + 1e-8)
        
        df['spread_bps'] = (df['spread'] / df['mid_price']) * 10000
        df['relative_spread'] = df['spread'] / df['mid_price']
        
        df['size_imbalance'] = (df['best_bid_size'] - df['best_ask_size']) / \
                                (df['best_bid_size'] + df['best_ask_size'] + 1e-8)
        
        df['total_top_size'] = df['best_bid_size'] + df['best_ask_size']
        df['log_total_size'] = cudf.log(df['total_top_size'] + 1e-8)
        
        elapsed = time.time() - start
        print(f"   ‚úì Added 6 basic features in {elapsed:.3f}s")
        return df
    
    def compute_returns(self, df, windows=[1, 5, 10, 30, 60, 300]):
        """Multi-timeframe returns (GPU-accelerated GroupBy)"""
        print(f"\nüìà Computing returns at {len(windows)} timeframes (GPU)...")
        start = time.time()
        
        price_col = 'microprice' if 'microprice' in df.columns else 'mid_price'
        
        for window in windows:
            # cuDF groupby is GPU-accelerated
            df[f'return_{window}s'] = df.groupby('product_id')[price_col].pct_change(window)
            df[f'log_return_{window}s'] = df.groupby('product_id')[price_col].apply(
                lambda x: cudf.log(x / x.shift(window))
            )
        
        elapsed = time.time() - start
        print(f"   ‚úì Added {len(windows) * 2} return features in {elapsed:.3f}s")
        print(f"   ‚úì GPU speedup estimate: {elapsed*10:.1f}s with pandas")
        return df
    
    def compute_volatility(self, df, windows=[10, 30, 60, 300]):
        """Volatility estimation (GPU rolling windows)"""
        print(f"\nüìä Computing volatility features (GPU)...")
        start = time.time()
        
        for window in windows:
            # Standard volatility
            if 'return_1s' in df.columns:
                df[f'volatility_{window}s'] = df.groupby('product_id')['return_1s'].rolling(
                    window, min_periods=1
                ).std().reset_index(0, drop=True)
            
            # Parkinson estimator (GPU math operations)
            log_hl = cudf.log(df['best_ask'] / df['best_bid'])
            df[f'parkinson_vol_{window}s'] = df.groupby('product_id').apply(
                lambda g: cudf.sqrt(
                    1 / (4 * cp.log(2)) * 
                    (cudf.log(g['best_ask'] / g['best_bid'])**2).rolling(window, min_periods=1).mean()
                )
            ).reset_index(0, drop=True)
        
        elapsed = time.time() - start
        print(f"   ‚úì Added {len(windows) * 2} volatility features in {elapsed:.3f}s")
        return df
    
    def compute_order_flow_features(self, df, windows=[10, 30, 60]):
        """Order flow features (GPU rolling)"""
        print(f"\nüíß Computing order flow features (GPU)...")
        start = time.time()
        
        for window in windows:
            df[f'order_imbalance_ma_{window}'] = df.groupby('product_id')['order_imbalance'].rolling(
                window, min_periods=1
            ).mean().reset_index(0, drop=True)
            
            if 'total_depth' in df.columns:
                df[f'depth_change_{window}'] = df.groupby('product_id')['total_depth'].pct_change(window)
        
        elapsed = time.time() - start
        print(f"   ‚úì Added {len(windows) * 2} order flow features in {elapsed:.3f}s")
        return df
    
    def compute_vpin(self, df):
        """VPIN toxicity metric (GPU)"""
        print(f"\nüß™ Computing VPIN (GPU)...")
        start = time.time()
        
        if 'order_imbalance' not in df.columns:
            print("   ‚ö†Ô∏è  order_imbalance not found, skipping")
            return df
        
        # GPU-accelerated rolling abs
        df['vpin'] = df.groupby('product_id')['order_imbalance'].rolling(
            self.vpin_window, min_periods=5
        ).apply(lambda x: cudf.abs(x).mean()).reset_index(0, drop=True)
        
        df['vpin_trend'] = df.groupby('product_id')['vpin'].diff(5)
        
        elapsed = time.time() - start
        print(f"   ‚úì Added VPIN features in {elapsed:.3f}s")
        return df
    
    def compute_microstructure_features(self, df):
        """Microstructure features (GPU)"""
        print(f"\nüî¨ Computing microstructure features (GPU)...")
        start = time.time()
        
        df['price_change'] = df.groupby('product_id')['mid_price'].diff()
        
        # Roll spread (custom rolling function on GPU)
        def roll_spread_calc(x):
            if len(x) < 2:
                return 0
            cov = cp.cov(cp.array(x[:-1]), cp.array(x[1:]))[0, 1]
            return 2 * cp.sqrt(-cov) if cov < 0 else 0
        
        df['roll_spread'] = df.groupby('product_id')['price_change'].rolling(
            20, min_periods=5
        ).apply(roll_spread_calc).reset_index(0, drop=True)
        
        if 'microprice' in df.columns:
            df['effective_spread'] = 2 * cudf.abs(df['mid_price'] - df['microprice'])
        
        elapsed = time.time() - start
        print(f"   ‚úì Added microstructure features in {elapsed:.3f}s")
        return df
    
    def compute_technical_indicators(self, df):
        """Technical indicators (GPU-accelerated)"""
        print(f"\nüìä Computing technical indicators (GPU)...")
        start = time.time()
        
        price_col = 'microprice' if 'microprice' in df.columns else 'mid_price'
        
        # Moving Averages (GPU rolling)
        for window in [5, 10, 20, 50]:
            df[f'SMA_{window}'] = df.groupby('product_id')[price_col].transform(
                lambda x: x.rolling(window, min_periods=1).mean()
            )
        
        # Exponential Moving Averages
        for span in [9, 12, 21, 26]:
            df[f'EMA_{span}'] = df.groupby('product_id')[price_col].transform(
                lambda x: x.ewm(span=span, adjust=False).mean()
            )
        
        # RSI (GPU vectorized)
        def calculate_rsi_gpu(prices, period=14):
            delta = prices.diff()
            gain = delta.where(delta > 0, 0).rolling(window=period, min_periods=1).mean()
            loss = -delta.where(delta < 0, 0).rolling(window=period, min_periods=1).mean()
            rs = gain / (loss + 1e-8)
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        df['RSI_14'] = df.groupby('product_id')[price_col].transform(
            lambda x: calculate_rsi_gpu(x, period=14)
        )
        df['RSI_7'] = df.groupby('product_id')[price_col].transform(
            lambda x: calculate_rsi_gpu(x, period=7)
        )
        
        # Bollinger Bands
        df['BB_middle'] = df['SMA_20']
        df['BB_std'] = df.groupby('product_id')[price_col].transform(
            lambda x: x.rolling(20, min_periods=1).std()
        )
        df['BB_upper'] = df['BB_middle'] + 2 * df['BB_std']
        df['BB_lower'] = df['BB_middle'] - 2 * df['BB_std']
        df['BB_width'] = (df['BB_upper'] - df['BB_lower']) / df['BB_middle']
        df['BB_position'] = (df[price_col] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'] + 1e-8)
        
        # MACD
        df['MACD'] = df['EMA_12'] - df['EMA_26']
        df['MACD_signal'] = df.groupby('product_id')['MACD'].transform(
            lambda x: x.ewm(span=9, adjust=False).mean()
        )
        df['MACD_histogram'] = df['MACD'] - df['MACD_signal']
        
        # Price position indicators
        df['price_above_SMA20'] = (df[price_col] > df['SMA_20']).astype('int32')
        df['price_above_SMA50'] = (df[price_col] > df['SMA_50']).astype('int32')
        df['SMA_crossover_5_20'] = (df['SMA_5'] > df['SMA_20']).astype('int32')
        df['EMA_crossover_9_21'] = (df['EMA_9'] > df['EMA_21']).astype('int32')
        
        # Distance from MAs
        df['distance_from_SMA20'] = (df[price_col] - df['SMA_20']) / df['SMA_20']
        df['distance_from_EMA21'] = (df[price_col] - df['EMA_21']) / df['EMA_21']
        
        # Stochastic Oscillator
        def calculate_stochastic_gpu(prices, period=14):
            low_min = prices.rolling(window=period, min_periods=1).min()
            high_max = prices.rolling(window=period, min_periods=1).max()
            stoch = 100 * (prices - low_min) / (high_max - low_min + 1e-8)
            return stoch
        
        df['Stochastic_14'] = df.groupby('product_id')[price_col].transform(
            lambda x: calculate_stochastic_gpu(x, period=14)
        )
        
        # ATR
        if 'best_bid' in df.columns and 'best_ask' in df.columns:
            df['true_range'] = df['best_ask'] - df['best_bid']
            df['ATR_14'] = df.groupby('product_id')['true_range'].transform(
                lambda x: x.rolling(14, min_periods=1).mean()
            )
            df['ATR_normalized'] = df['ATR_14'] / df[price_col]
        
        elapsed = time.time() - start
        print(f"   ‚úì Added 30+ technical indicators in {elapsed:.3f}s")
        print(f"   ‚úì GPU speedup estimate: {elapsed*3:.1f}s with pandas")
        return df
    
    def compute_cross_product_features(self, df):
        """Cross-product correlation (GPU pivot)"""
        print(f"\nüîó Computing cross-product features (GPU)...")
        start = time.time()
        
        if len(df['product_id'].unique()) < 2:
            print("   ‚ö†Ô∏è  Single product, skipping")
            return df
        
        # GPU pivot
        price_pivot = df.pivot_table(
            index='timestamp',
            columns='product_id',
            values='mid_price'
        )
        
        if len(price_pivot.columns) >= 2:
            products = price_pivot.columns.to_list()
            
            # GPU rolling correlation
            window = 100
            correlation = price_pivot[products[0]].rolling(window).corr(
                price_pivot[products[1]]
            )
            
            # Merge back (GPU merge)
            corr_df = cudf.DataFrame({
                'timestamp': price_pivot.index,
                'btc_eth_correlation': correlation
            })
            
            df = df.merge(corr_df, on='timestamp', how='left')
            
            elapsed = time.time() - start
            print(f"   ‚úì Added cross-product correlation in {elapsed:.3f}s")
        
        return df
    
    def compute_target_labels(self, df):
        """Multi-horizon targets (GPU merge_asof)"""
        print(f"\nüéØ Computing target labels for {len(self.prediction_horizons)} horizons (GPU)...")
        
        price_col = 'microprice' if 'microprice' in df.columns else 'mid_price'
        
        for horizon in self.prediction_horizons:
            print(f"   Computing {horizon}s ahead targets...")
            horizon_start = time.time()
            
            for product in df['product_id'].unique().to_pandas():
                product_mask = df['product_id'] == product
                product_df = df[product_mask].copy()
                
                # Create future price (GPU operations)
                future_df = product_df[['timestamp', price_col]].copy()
                future_df['timestamp'] = future_df['timestamp'] - cudf.Timedelta(seconds=horizon)
                future_df = future_df.rename(columns={price_col: f'future_price_{horizon}s'})
                
                # GPU merge_asof
                product_df = cudf.merge_asof(
                    product_df.sort_values('timestamp'),
                    future_df.sort_values('timestamp'),
                    on='timestamp',
                    direction='forward'
                )
                
                # GPU calculation
                product_df[f'target_return_{horizon}s'] = (
                    (product_df[f'future_price_{horizon}s'] - product_df[price_col]) / 
                    product_df[price_col]
                )
                
                # Direction labels
                for threshold in self.return_thresholds:
                    threshold_pct = threshold * 100
                    # Use GPU conditional operations
                    returns = product_df[f'target_return_{horizon}s']
                    direction = cudf.Series(['neutral'] * len(returns))
                    direction[returns < -threshold] = 'down'
                    direction[returns > threshold] = 'up'
                    product_df[f'target_direction_{horizon}s_{threshold_pct:.1f}pct'] = direction
                
                # Update main dataframe (GPU operation)
                df.loc[product_mask, product_df.columns] = product_df
            
            elapsed = time.time() - horizon_start
            print(f"      ‚úì Completed in {elapsed:.2f}s")
        
        print(f"   ‚úì Added {len(self.prediction_horizons) * (1 + len(self.return_thresholds))} target columns")
        return df
    
    def engineer_features(self, snapshots_csv, output_csv='datasets/crypto_features_gpu.csv'):
        """Main GPU-accelerated feature engineering pipeline"""
        
        print(f"\n{'='*80}")
        print(f"üöÄ GPU-ACCELERATED ML FEATURE ENGINEERING")
        print(f"{'='*80}")
        print(f"\nInput: {snapshots_csv}")
        print(f"Output: {output_csv}")
        
        overall_start = time.time()
        
        # Load with cuDF (GPU)
        print(f"\nüìñ Loading snapshots (GPU)...")
        load_start = time.time()
        
        df = cudf.read_csv(snapshots_csv)
        df['timestamp'] = cudf.to_datetime(df['timestamp'])
        df = df.sort_values(['product_id', 'timestamp'])
        
        load_time = time.time() - load_start
        
        print(f"   ‚úì Loaded {len(df):,} snapshots in {load_time:.2f}s")
        print(f"   ‚úì Products: {df['product_id'].unique().to_pandas().tolist()}")
        print(f"   ‚úì GPU speedup estimate: {load_time*5:.1f}s with pandas")
        
        initial_cols = len(df.columns)
        
        # Feature engineering pipeline (all on GPU)
        df = self.compute_basic_features(df)
        df = self.compute_returns(df)
        df = self.compute_volatility(df)
        df = self.compute_order_flow_features(df)
        df = self.compute_vpin(df)
        df = self.compute_microstructure_features(df)
        df = self.compute_technical_indicators(df)  # NEW: Technical analysis
        df = self.compute_cross_product_features(df)
        df = self.compute_target_labels(df)
        
        final_cols = len(df.columns)
        
        # Quality filters (GPU)
        print(f"\nüîç Applying quality filters (GPU)...")
        filter_start = time.time()
        
        initial_rows = len(df)
        
        # Remove missing targets (GPU boolean indexing)
        target_cols = [f'target_return_{h}s' for h in self.prediction_horizons]
        df = df.dropna(subset=target_cols)
        
        # Spread filter (GPU boolean indexing)
        if self.min_spread_filter:
            df = df[df['spread_bps'] >= 1.0]
        
        final_rows = len(df)
        filter_time = time.time() - filter_start
        
        print(f"   ‚úì Filtered {initial_rows - final_rows:,} rows in {filter_time:.2f}s")
        print(f"   ‚úì Remaining: {final_rows:,} samples")
        
        # Save (GPU write)
        print(f"\nüíæ Saving features (GPU)...")
        save_start = time.time()
        
        df.to_csv(output_csv, index=False)
        
        save_time = time.time() - save_start
        overall_time = time.time() - overall_start
        
        print(f"\n{'='*80}")
        print(f"‚úÖ SUCCESS!")
        print(f"{'='*80}")
        
        print(f"\nPerformance:")
        print(f"  Total time: {overall_time:.1f}s ({overall_time/60:.1f} minutes)")
        print(f"  Load time: {load_time:.2f}s")
        print(f"  Feature engineering: {overall_time - load_time - save_time:.2f}s")
        print(f"  Save time: {save_time:.2f}s")
        print(f"  Estimated pandas time: {overall_time*15:.0f}s ({overall_time*15/60:.1f} minutes)")
        print(f"  GPU speedup: ~{15:.0f}x")
        
        print(f"\nOutput:")
        print(f"  File: {output_csv}")
        print(f"  Features added: {final_cols - initial_cols}")
        print(f"  Total columns: {final_cols}")
        print(f"  Total samples: {final_rows:,}")
        
        # Transfer to CPU for memory calculation
        df_pandas = df.to_pandas()
        memory_mb = df_pandas.memory_usage(deep=True).sum() / 1024**2
        print(f"  Memory: {memory_mb:.1f} MB")
        
        print(f"\nüìä Feature summary:")
        feature_types = {
            'Basic': ['spread', 'imbalance', 'size'],
            'Returns': ['return_', 'log_return_'],
            'Volatility': ['volatility_', 'parkinson_'],
            'Order Flow': ['order_imbalance_ma', 'depth_change'],
            'Microstructure': ['vpin', 'roll_spread', 'effective_spread'],
            'Technical Indicators': ['SMA_', 'EMA_', 'RSI_', 'MACD', 'BB_', 'Stochastic', 'ATR'],
            'Cross-product': ['correlation'],
            'Targets': ['target_']
        }
        
        for ftype, keywords in feature_types.items():
            count = sum(1 for col in df.columns if any(kw in col for kw in keywords))
            print(f"  {ftype}: {count} features")
        
        print(f"\n{'='*80}\n")
        
        return df


def main():
    """Execute GPU-accelerated Stage 3"""
    
    print("Checking GPU availability...")
    try:
        import cudf
        print(f"‚úì cuDF version: {cudf.__version__}")
        print(f"‚úì GPU detected!")
    except ImportError:
        print("‚ùå cuDF not installed!")
        print("Install with: conda install -c rapidsai -c conda-forge -c nvidia rapids")
        return
    
    SNAPSHOTS_CSV = 'datasets/market_snapshots.csv'
    OUTPUT_CSV = 'datasets/crypto_features_gpu.csv'
    
    engineer = AdvancedFeatureEngineerGPU(
        prediction_horizons=[30, 60, 300],
        return_thresholds=[0.001, 0.002, 0.005],
        vpin_window=50,
        min_spread_filter=True
    )
    
    features_df = engineer.engineer_features(SNAPSHOTS_CSV, OUTPUT_CSV)
    
    print("üéâ GPU-accelerated Stage 3 complete!\n")


if __name__ == '__main__':
    main()
